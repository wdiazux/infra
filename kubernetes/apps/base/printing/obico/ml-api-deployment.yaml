# Obico ML API
#
# Machine learning service for failure detection.
# Uses custom CUDA 12 image for GPU compatibility.
# Build custom image: cd docker/obico-ml-api && ./build.sh --push
apiVersion: apps/v1
kind: Deployment
metadata:
  name: obico-ml-api
  namespace: printing
  labels:
    app.kubernetes.io/name: obico-ml-api
    app.kubernetes.io/component: ml
    app.kubernetes.io/part-of: printing
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      app.kubernetes.io/name: obico-ml-api
  template:
    metadata:
      labels:
        app.kubernetes.io/name: obico-ml-api
        app.kubernetes.io/component: ml
        app.kubernetes.io/part-of: printing
    spec:
      runtimeClassName: nvidia
      containers:
        - name: ml-api
          # Custom image with CUDA 12 support (models included)
          # Build: cd docker/obico-ml-api && ./build.sh --push
          image: ghcr.io/wdiazux/obico-ml-api:cuda12.3
          imagePullPolicy: Always
          ports:
            - name: http
              containerPort: 3333
              protocol: TCP
          env:
            - name: TZ
              value: "America/El_Salvador"
            - name: DEBUG
              value: "False"
            - name: FLASK_APP
              value: "server.py"
            - name: HAS_GPU
              value: "True"
            - name: NVIDIA_VISIBLE_DEVICES
              value: "all"
          resources:
            requests:
              memory: 512Mi
              nvidia.com/gpu: "1"
            limits:
              memory: 4Gi
              nvidia.com/gpu: "1"
          livenessProbe:
            httpGet:
              path: /hc/
              port: http
            initialDelaySeconds: 120
            periodSeconds: 30
            timeoutSeconds: 10
          readinessProbe:
            httpGet:
              path: /hc/
              port: http
            initialDelaySeconds: 60
            periodSeconds: 10
