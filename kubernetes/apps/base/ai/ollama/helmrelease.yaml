# Ollama HelmRelease
#
# LLM inference backend with NVIDIA GPU support.
# Chart: https://github.com/otwld/ollama-helm
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: ollama
  namespace: ai
spec:
  interval: 30m
  timeout: 15m
  chart:
    spec:
      chart: ollama
      version: "1.39.0"
      sourceRef:
        kind: HelmRepository
        name: ollama
        namespace: flux-system
      interval: 12h
  install:
    remediation:
      retries: 3
  upgrade:
    cleanupOnFail: true
    remediation:
      retries: 3
  values:
    image:
      tag: "0.15.2"

    ollama:
      gpu:
        enabled: true
        type: nvidia
        number: 1
      mountPath: /root/.ollama

    runtimeClassName: nvidia

    extraEnv:
      - name: TZ
        value: "America/El_Salvador"
      - name: OLLAMA_NUM_PARALLEL
        value: "2"
      - name: OLLAMA_MAX_LOADED_MODELS
        value: "1"
      - name: OLLAMA_KEEP_ALIVE
        value: "15m"
      - name: OLLAMA_FLASH_ATTENTION
        value: "1"

    persistentVolume:
      enabled: true
      existingClaim: nfs-ai-models
      subPath: ollama

    resources:
      requests:
        memory: 100Mi
      limits:
        memory: 32Gi

    service:
      type: ClusterIP
      port: 11434
