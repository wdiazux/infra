# Longhorn Helm Values for Talos Linux Single-Node Cluster
# This configuration optimizes Longhorn as the PRIMARY storage solution
# for almost all services in the cluster.
#
# Version: Longhorn 1.7.x+
# Talos Compatibility: v1.8+
# Last Updated: 2025-11-22
# OPTIMIZED: Performance tuning for single-node setup added

# Default Settings
defaultSettings:
  # CRITICAL: Must match Terraform kubelet extraMounts configuration
  # Terraform mounts /var/lib/longhorn with rshared propagation (main.tf line 120)
  defaultDataPath: /var/lib/longhorn

  # Single-node configuration: Use 1 replica
  # IMPORTANT: When expanding to 3 nodes, change this to 3
  defaultReplicaCount: 1

  # Storage reservation: Keep 25% free for system operations
  storageReservedPercentageForDefaultDisk: 25

  # Disable replica soft anti-affinity for single-node
  # This allows multiple replicas on the same node (when expanding)
  replicaSoftAntiAffinity: "false"

  # Automatically create default disk on labeled nodes
  createDefaultDiskLabeledNodes: true

  # Backup configuration (configure later with NAS or S3)
  # backupTarget: "nfs://192.168.1.100:/longhorn-backups"
  # backupTargetCredentialSecret: "longhorn-backup-secret"

  # Snapshot settings
  snapshotDataIntegrity: "enabled"
  snapshotMaxCount: 10

  # Automatic volume cleanup
  orphanAutoDeletion: true

  # Performance tuning
  storageMinimalAvailablePercentage: 10
  guaranteedInstanceManagerCPU: 5

  # Volume attachment settings
  disableSchedulingOnCordonedNode: true
  replicaAutoBalance: "best-effort"

  # Allow node drain with attached volumes (useful for single node)
  nodeDownPodDeletionPolicy: "delete-both-statefulset-and-deployment-pod"

  # Upgrade strategy
  concurrentAutomaticEngineUpgradePerNodeLimit: 1

  # ========================================================================
  # PERFORMANCE OPTIMIZATION (ADDED for single-node setup)
  # ========================================================================
  # These settings prevent I/O saturation and resource contention on single-node
  # Verified against Longhorn v1.7.2 documentation

  # Limit concurrent replica rebuilds to prevent I/O saturation
  # Default: 0 (no limit) - Changed to 2 for single-node
  # This ensures only 2 volumes can rebuild simultaneously
  concurrentReplicaRebuildPerNodeLimit: 2

  # Limit concurrent backup/restore operations
  # Default: 0 (no limit) - Changed to 2 for single-node
  # Prevents backup operations from overwhelming storage I/O
  concurrentVolumeBackupRestorePerNodeLimit: 2

  # Wait time before reusing failed replica data
  # Default: 600 seconds (10 minutes) - Reduced to 300 (5 minutes)
  # Faster recovery from failed replicas on single node
  replicaReplenishmentWaitInterval: 300

  # Backup worker thread limits (if using NFS/S3 backup target)
  # Default: 2 - Kept at 2 for single-node
  backupConcurrentLimit: 2
  restoreConcurrentLimit: 2

  # Enable fast replica rebuild using checksums (performance improvement)
  # Default: false - Changed to true for faster rebuilds
  fastReplicaRebuildEnabled: true

# Persistence settings
persistence:
  # Make Longhorn the default storage class
  defaultClass: true
  defaultFsType: ext4
  defaultMkfsParams: ""
  defaultClassReplicaCount: 1
  defaultDataLocality: best-effort
  reclaimPolicy: Delete
  migratable: false
  recurringJobSelector:
    enable: false

# Ingress for Longhorn UI (optional - configure if using ingress controller)
ingress:
  enabled: false
  # Enable and configure if you have Cilium ingress or nginx-ingress
  # host: longhorn.yourdomain.com
  # ingressClassName: cilium
  # tls: true
  # tlsSecret: longhorn-tls

# Service configuration
service:
  ui:
    type: ClusterIP
    nodePort: null
  manager:
    type: ClusterIP
    nodePort: null

# Enable CSI Driver (required for dynamic provisioning)
csi:
  attacherReplicaCount: 1
  provisionerReplicaCount: 1
  resizerReplicaCount: 1
  snapshotterReplicaCount: 1

# Longhorn Manager (controls Longhorn operations)
longhornManager:
  priorityClass: ~
  tolerations: []
  nodeSelector: {}

# Longhorn Driver (CSI driver pods)
longhornDriver:
  priorityClass: ~
  tolerations: []
  nodeSelector: {}

# Longhorn UI
longhornUI:
  replicas: 1
  priorityClass: ~
  tolerations: []
  nodeSelector: {}

# Resource limits (adjust based on your 96GB RAM system)
# These are conservative defaults - can be increased if needed
resources:
  limits:
    cpu: 1000m
    memory: 2Gi
  requests:
    cpu: 500m
    memory: 1Gi

# Enable metrics for Prometheus monitoring
metrics:
  serviceMonitor:
    enabled: false
    # Set to true if you have kube-prometheus-stack installed

# Private registry settings (if using private registry)
privateRegistry:
  createSecret: false

# Upgrade strategy
upgradeStrategy:
  type: RollingUpdate

# Network policies (optional - enable if using network policies)
networkPolicies:
  enabled: false
  type: ""

# Enable OpenShift-specific features (keep disabled for Talos)
enablePSP: false
enableGoCoverDir: false

# Annotations for all resources
annotations: {}

# Global node selector (leave empty for single node)
global:
  nodeSelector: {}
  tolerations: []

# Default StorageClass parameters
# These apply to the default storage class created by Longhorn
defaultStorageClass:
  parameters:
    numberOfReplicas: "1"
    staleReplicaTimeout: "30"
    fromBackup: ""
    fsType: "ext4"
    dataLocality: "best-effort"
    # For single node, disable migration
    migratable: "false"

# ============================================================================
# CHANGELOG - OPTIMIZED Version
# ============================================================================
#
# Changes from original configuration:
#
# 1. Added Performance Optimization Section (lines 54-76):
#    - concurrentReplicaRebuildPerNodeLimit: 2
#      * Prevents multiple simultaneous rebuilds from saturating I/O
#      * Default was 0 (unlimited), changed for single-node stability
#
#    - concurrentVolumeBackupRestorePerNodeLimit: 2
#      * Controls concurrent backup/restore operations
#      * Default was 0 (unlimited), changed for single-node stability
#
#    - replicaReplenishmentWaitInterval: 300
#      * Reduced from 600s (10 min) to 300s (5 min)
#      * Faster recovery from failed replicas on single node
#
#    - backupConcurrentLimit: 2
#    - restoreConcurrentLimit: 2
#      * Explicit limits for backup/restore worker threads
#      * Prevents backup operations from overwhelming storage
#
#    - fastReplicaRebuildEnabled: true
#      * Enable fast rebuild using checksums for better performance
#      * Default was false, enabled for optimization
#
# All added settings verified against:
# - Longhorn v1.7.2 Settings Reference: https://longhorn.io/docs/1.7.2/references/settings/
# - Longhorn Best Practices: https://longhorn.io/docs/1.7.1/best-practices/
# - Longhorn Performance Tuning: https://support.tools/training/longhorn/performance/
#
# ============================================================================
# USAGE NOTES
# ============================================================================
#
# Installation:
#   helm install longhorn longhorn/longhorn \
#     --namespace longhorn-system \
#     --create-namespace \
#     --values longhorn-values-OPTIMIZED.yaml
#
# Upgrade from existing installation:
#   helm upgrade longhorn longhorn/longhorn \
#     --namespace longhorn-system \
#     --values longhorn-values-OPTIMIZED.yaml
#
# Verify settings applied:
#   kubectl port-forward -n longhorn-system svc/longhorn-frontend 8080:80
#   # Open http://localhost:8080
#   # Navigate to Settings and verify:
#   # - Concurrent Replica Rebuild Per Node Limit: 2
#   # - Concurrent Volume Backup Restore Per Node Limit: 2
#   # - Replica Replenishment Wait Interval: 300
#   # - Fast Replica Rebuild Enabled: true
#
# ============================================================================
# MIGRATION PATH TO 3-NODE HA
# ============================================================================
#
# When expanding to 3-node cluster:
# 1. Change defaultReplicaCount: 1 -> 3
# 2. Change replicaSoftAntiAffinity: "false" -> "true"
# 3. Update storage classes to use numberOfReplicas: "3"
# 4. Consider increasing concurrent rebuild limits to 3-5
# 5. Enable volume migration: migratable: "false" -> "true"
#
# ============================================================================
