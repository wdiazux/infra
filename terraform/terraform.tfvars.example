# Terraform Variables Example for Talos Linux on Proxmox
#
# Copy this file to terraform.tfvars and fill in your actual values
# cp terraform.tfvars.example terraform.tfvars
#
# NOTE: terraform.tfvars is in .gitignore to prevent committing credentials

# ============================================================================
# Proxmox Connection (Required)
# ============================================================================

proxmox_url      = "https://10.10.2.2:8006/api2/json"
proxmox_username = "root@pve"
proxmox_node     = "pve"

# Use API token (recommended)
proxmox_api_token = "PVEAPIToken=terraform@pve!terraform-token=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Alternatively, use environment variable (more secure):
# export TF_VAR_proxmox_api_token="PVEAPIToken=terraform@pve!terraform-token=..."
# Then omit proxmox_api_token line above

# Or use password (less secure, but required for some GPU passthrough operations)
# proxmox_password = "your-password-here"

# Skip TLS verification for self-signed certs (homelab)
proxmox_insecure = true

# ============================================================================
# Talos Template (Required - created by Packer)
# ============================================================================

talos_template_name = "talos-1.12.1-nvidia-template"
talos_version       = "v1.12.1"
kubernetes_version  = "v1.31.0"

# CRITICAL FOR LONGHORN: Talos Factory Schematic ID
# Generate at: https://factory.talos.dev/
# Required extensions for Longhorn storage manager:
#   - siderolabs/iscsi-tools (required)
#   - siderolabs/util-linux-tools (required)
#   - siderolabs/qemu-guest-agent (recommended for Proxmox)
# Optional extensions for GPU workloads:
#   - nonfree-kmod-nvidia-production
#   - nvidia-container-toolkit-production
#
# Example schematic ID format: "376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba"
talos_schematic_id = ""  # CHANGE ME - paste your schematic ID here

# Steps to generate schematic ID:
# 1. Visit https://factory.talos.dev/
# 2. Select Talos version (v1.12.1)
# 3. Add extensions:
#    - siderolabs/iscsi-tools
#    - siderolabs/util-linux-tools
#    - siderolabs/qemu-guest-agent
#    - nonfree-kmod-nvidia-production (if using GPU)
#    - nvidia-container-toolkit-production (if using GPU)
# 4. Click "Generate" and copy the schematic ID
# 5. Paste the ID above

# ============================================================================
# Cluster Configuration
# ============================================================================

cluster_name = "homelab-k8s"

# Cluster endpoint will be set to node_ip if not specified
# cluster_endpoint = "https://10.10.2.10:6443"

# ============================================================================
# Node Configuration (Required)
# ============================================================================

node_name = "talos-node"
node_vm_id = 1000  # Talos VMs use range 1000-1999 (avoid conflict with traditional VMs)

# Static IP configuration (MUST match your network)
node_ip      = "10.10.2.10"  # Change to your desired IP
node_gateway = "10.10.2.1"    # Your router/gateway
node_netmask = 24               # Usually 24 for 255.255.255.0

# ============================================================================
# Hardware Resources
# ============================================================================

# CPU configuration
node_cpu_cores   = 8        # Adjust based on your needs
node_cpu_sockets = 1
node_cpu_type    = "host"   # MUST be 'host' for Talos

# Memory configuration
node_memory = 32768  # 32GB (32768 MB) - adjust as needed

# Disk configuration
node_disk_size    = 200          # 200GB for OS + containers + ephemeral
node_disk_storage = "tank"  # Your Proxmox storage pool name

# ============================================================================
# GPU Passthrough
# ============================================================================

# Enable NVIDIA GPU passthrough
enable_gpu_passthrough = true

# Find your GPU PCI ID with: lspci | grep -i nvidia
# This system: 07:00.0 VGA compatible controller: NVIDIA Corporation AD104GL [RTX 4000 SFF Ada Generation]
# Use "07:00" format (will be converted to "0000:07:00.0" internally)
gpu_pci_id = "07:00"

# PCIe passthrough mode (recommended)
gpu_pcie = true

# GPU ROM bar (false recommended for GPU passthrough)
gpu_rombar = false

# GPU Resource Mapping name (create in Proxmox: Datacenter → Resource Mappings → Add → PCI Device)
# Map 0000:07:00.0 with "All Functions" checked to include audio (07:00.1)
gpu_mapping = "nvidia-gpu"

# ============================================================================
# Network Configuration
# ============================================================================

network_bridge = "vmbr0"  # Proxmox bridge name
network_vlan   = 0        # 0 for no VLAN, or your VLAN ID
network_model  = "virtio"

# DNS servers
dns_servers = ["8.8.8.8", "8.8.4.4"]  # Google DNS, or use your own

# NTP servers
ntp_servers = ["time.cloudflare.com"]

# ============================================================================
# External Storage (NFS) - For Longhorn Backups (OPTIONAL)
# ============================================================================

# PRIMARY STORAGE: Longhorn (local cluster storage)
# BACKUP STORAGE: External NAS via NFS (optional, can be configured later)

# External NAS for Longhorn backup target
# Leave empty to configure later via Longhorn UI
nfs_server = ""  # Your NAS IP (e.g., "10.10.2.5")
nfs_path   = ""  # NFS export path (e.g., "/mnt/tank/longhorn-backups")

# You can add NFS backups later:
# 1. Set up NFS share on your NAS
# 2. Configure in Longhorn UI: Settings → General → Backup Target
# 3. Or update these values and run: helm upgrade longhorn ...

# ============================================================================
# Feature Flags
# ============================================================================

# Automatically bootstrap the cluster after creation
auto_bootstrap = true

# Generate kubeconfig file
generate_kubeconfig = true

# Allow pod scheduling on control plane (required for single-node)
allow_scheduling_on_control_plane = true

# Enable QEMU guest agent (should match template)
enable_qemu_agent = true

# Install Cilium CNI (requires manual installation after bootstrap)
install_cilium = true
cilium_version = "1.18.0"

# ============================================================================
# Tags and Metadata
# ============================================================================

tags = ["talos", "kubernetes", "nvidia-gpu", "homelab"]

description = "Talos Linux single-node Kubernetes cluster with NVIDIA GPU support"

# ============================================================================
# Advanced Configuration
# ============================================================================

# Additional Talos configuration patches (advanced users)
# talos_config_patches = [
#   yamlencode({
#     machine = {
#       sysctls = {
#         "vm.swappiness" = "10"
#       }
#     }
#   })
# ]

# Install disk (usually /dev/sda for first disk)
install_disk = "/dev/sda"

# Notes:
# - CRITICAL: Generate and set talos_schematic_id with Longhorn extensions
# - Update node_ip, node_gateway, node_netmask to match your network
# - nfs_server and nfs_path are OPTIONAL (can configure later via Longhorn UI)
# - Update gpu_pci_id based on `lspci` output on your Proxmox host
# - Verify talos_template_name matches the template created by Packer
# - Check node_disk_storage matches your Proxmox storage pool name
# - For multiple nodes, create separate .tfvars files (e.g., node1.tfvars, node2.tfvars)
# - GPU passthrough requires IOMMU enabled in BIOS and GRUB configuration
# - Single-node cluster requires allow_scheduling_on_control_plane = true
# - Longhorn kernel modules and kubelet mounts are auto-configured in main.tf

# ============================================================================
# Post-Deployment: Install Longhorn
# ============================================================================
# After successful `terraform apply`, install Longhorn storage manager:
#
# 1. Verify cluster is ready:
#    kubectl --kubeconfig=./kubeconfig get nodes
#
# 2. Create Longhorn namespace:
#    kubectl create namespace longhorn-system
#    kubectl label namespace longhorn-system pod-security.kubernetes.io/enforce=privileged
#
# 3. Install Longhorn via Helm:
#    helm repo add longhorn https://charts.longhorn.io
#    helm install longhorn longhorn/longhorn \
#      --namespace longhorn-system \
#      --values ../kubernetes/longhorn/longhorn-values.yaml
#
# 4. Apply storage classes:
#    kubectl apply -f ../kubernetes/storage-classes/longhorn-storage-classes.yaml
#
# 5. Verify installation:
#    kubectl get pods -n longhorn-system
#    kubectl get storageclass
#
# Full documentation: ../kubernetes/longhorn/INSTALLATION.md

# ============================================================================
# Traditional VMs Configuration (Optional)
# ============================================================================
# Deploy traditional Linux and Windows VMs from Packer templates
#
# IMPORTANT: VM definitions (CPU, memory, disk, IP, enabled/disabled) are
# configured in locals-vms.tf, NOT here. This file only contains shared
# variables like template names and cloud-init credentials.

# Common Configuration
# ----------------------------------------------------------------------------

common_tags     = ["homelab", "infra"]
default_storage = "tank"  # Proxmox storage pool for VM disks

# Cloud-init user configuration (applies to all Linux VMs)
cloud_init_user     = "admin"
cloud_init_password = "changeme"  # Change this!
# cloud_init_ssh_keys = ["ssh-rsa AAAA... user@host"]

# Windows credentials (uses Cloudbase-Init)
windows_admin_user     = "Administrator"
windows_admin_password = "ChangeMe123!"  # Change this!

# Network configuration
default_gateway = "10.10.2.1"
# dns_servers and dns_domain already set above for Talos

# Template Names (from Packer builds)
# ----------------------------------------------------------------------------
# Update these after running Packer builds
# Find templates in Proxmox UI or via: qm list

ubuntu_template_name  = "ubuntu-2404-cloud-template"
debian_template_name  = "debian-13-cloud-template"
arch_template_name    = "arch-linux-golden-template"
nixos_template_name   = "nixos-golden-template"
windows_template_name = "windows-11-golden-template"

# ============================================================================
# Traditional VMs Notes
# ============================================================================
#
# Template Names:
# - Update template names after running Packer builds
# - Packer adds timestamps: "ubuntu-24.04-golden-template-20251118-1234"
# - Find templates in Proxmox UI or via: qm list
#
# VM Configuration (in locals-vms.tf):
# - VMs are defined in locals-vms.tf using for_each pattern
# - Available VMs: ubuntu-dev (100), debian-prod (200), arch-dev (300),
#   nixos-lab (400), windows-desktop (500)
# - Add new VMs by adding entries in locals-vms.tf with unique name and vm_id
#
# Resource Allocation:
# - Total available: ~76GB RAM, 10-11 cores (after Proxmox + Talos)
# - Example: Talos=32GB+8c, Ubuntu=16GB+4c, Debian=16GB+4c, Arch=4GB+2c
# - Adjust based on workload requirements
# - See CLAUDE.md for detailed allocation examples
#
# IP Addresses:
# - DHCP: Automatic (requires DHCP server)
# - Static: "10.10.2.10/24" format (update gateway and netmask)
# - Ensure IPs don't conflict with existing devices
#
# Deployment:
# - Deploy all enabled VMs: terraform apply
# - Deploy specific VM: terraform apply -target='module.traditional_vm["ubuntu-dev"]'
# - Destroy specific VM: terraform destroy -target='module.traditional_vm["ubuntu-dev"]'
# - Enable/disable VMs: Set enabled = true/false in locals-vms.tf
#
# Cloud-init:
# - All Linux templates have cloud-init pre-installed
# - Windows uses Cloudbase-Init (Windows equivalent)
# - SSH keys recommended for Linux (password auth less secure)
# - Passwords are temporary - change after first login
