# Terraform Variables Example for Talos Linux on Proxmox
#
# Copy this file to terraform.tfvars and fill in your actual values
# cp terraform.tfvars.example terraform.tfvars
#
# NOTE: terraform.tfvars is in .gitignore to prevent committing credentials

# ============================================================================
# Proxmox Connection (Required)
# ============================================================================

proxmox_url      = "https://proxmox.local:8006/api2/json"
proxmox_username = "root@pam"
proxmox_node     = "pve"

# Use API token (recommended)
proxmox_api_token = "PVEAPIToken=terraform@pam!terraform-token=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Alternatively, use environment variable (more secure):
# export TF_VAR_proxmox_api_token="PVEAPIToken=terraform@pam!terraform-token=..."
# Then omit proxmox_api_token line above

# Or use password (less secure, but required for some GPU passthrough operations)
# proxmox_password = "your-password-here"

# Skip TLS verification for self-signed certs (homelab)
proxmox_insecure = true

# ============================================================================
# Talos Template (Required - created by Packer)
# ============================================================================

talos_template_name = "talos-1.11.4-nvidia-template"
talos_version       = "v1.11.4"
kubernetes_version  = "v1.31.0"

# CRITICAL FOR LONGHORN: Talos Factory Schematic ID
# Generate at: https://factory.talos.dev/
# Required extensions for Longhorn storage manager:
#   - siderolabs/iscsi-tools (required)
#   - siderolabs/util-linux-tools (required)
#   - siderolabs/qemu-guest-agent (recommended for Proxmox)
# Optional extensions for GPU workloads:
#   - nonfree-kmod-nvidia-production
#   - nvidia-container-toolkit-production
#
# Example schematic ID format: "376567988ad370138ad8b2698212367b8edcb69b5fd68c80be1f2ec7d603b4ba"
talos_schematic_id = ""  # CHANGE ME - paste your schematic ID here

# Steps to generate schematic ID:
# 1. Visit https://factory.talos.dev/
# 2. Select Talos version (v1.11.4)
# 3. Add extensions:
#    - siderolabs/iscsi-tools
#    - siderolabs/util-linux-tools
#    - siderolabs/qemu-guest-agent
#    - nonfree-kmod-nvidia-production (if using GPU)
#    - nvidia-container-toolkit-production (if using GPU)
# 4. Click "Generate" and copy the schematic ID
# 5. Paste the ID above

# ============================================================================
# Cluster Configuration
# ============================================================================

cluster_name = "homelab-k8s"

# Cluster endpoint will be set to node_ip if not specified
# cluster_endpoint = "https://192.168.1.100:6443"

# ============================================================================
# Node Configuration (Required)
# ============================================================================

node_name = "talos-node"
node_vm_id = 1000  # Talos VMs use range 1000-1999 (avoid conflict with traditional VMs)

# Static IP configuration (MUST match your network)
node_ip      = "192.168.1.100"  # Change to your desired IP
node_gateway = "192.168.1.1"    # Your router/gateway
node_netmask = 24               # Usually 24 for 255.255.255.0

# ============================================================================
# Hardware Resources
# ============================================================================

# CPU configuration
node_cpu_cores   = 8        # Adjust based on your needs
node_cpu_sockets = 1
node_cpu_type    = "host"   # MUST be 'host' for Talos

# Memory configuration
node_memory = 32768  # 32GB (32768 MB) - adjust as needed

# Disk configuration
node_disk_size    = 200          # 200GB for OS + containers + ephemeral
node_disk_storage = "local-zfs"  # Your Proxmox storage pool name

# ============================================================================
# GPU Passthrough
# ============================================================================

# Enable NVIDIA GPU passthrough
enable_gpu_passthrough = true

# Find your GPU PCI ID with: lspci | grep -i nvidia
# Example output: 01:00.0 VGA compatible controller: NVIDIA Corporation ...
# Use "01:00" format (will be converted to "0000:01:00.0" internally)
gpu_pci_id = "01:00"

# PCIe passthrough mode (recommended)
gpu_pcie = true

# GPU ROM bar (false recommended for GPU passthrough)
gpu_rombar = false

# ============================================================================
# Network Configuration
# ============================================================================

network_bridge = "vmbr0"  # Proxmox bridge name
network_vlan   = 0        # 0 for no VLAN, or your VLAN ID
network_model  = "virtio"

# DNS servers
dns_servers = ["8.8.8.8", "8.8.4.4"]  # Google DNS, or use your own

# NTP servers
ntp_servers = ["time.cloudflare.com"]

# ============================================================================
# External Storage (NFS) - For Longhorn Backups (OPTIONAL)
# ============================================================================

# PRIMARY STORAGE: Longhorn (local cluster storage)
# BACKUP STORAGE: External NAS via NFS (optional, can be configured later)

# External NAS for Longhorn backup target
# Leave empty to configure later via Longhorn UI
nfs_server = ""  # Your NAS IP (e.g., "192.168.1.200")
nfs_path   = ""  # NFS export path (e.g., "/mnt/tank/longhorn-backups")

# You can add NFS backups later:
# 1. Set up NFS share on your NAS
# 2. Configure in Longhorn UI: Settings → General → Backup Target
# 3. Or update these values and run: helm upgrade longhorn ...

# ============================================================================
# Feature Flags
# ============================================================================

# Automatically bootstrap the cluster after creation
auto_bootstrap = true

# Generate kubeconfig file
generate_kubeconfig = true

# Allow pod scheduling on control plane (required for single-node)
allow_scheduling_on_control_plane = true

# Enable QEMU guest agent (should match template)
enable_qemu_agent = true

# Install Cilium CNI (requires manual installation after bootstrap)
install_cilium = true
cilium_version = "1.18.0"

# ============================================================================
# Tags and Metadata
# ============================================================================

tags = ["talos", "kubernetes", "nvidia-gpu", "homelab"]

description = "Talos Linux single-node Kubernetes cluster with NVIDIA GPU support"

# ============================================================================
# Advanced Configuration
# ============================================================================

# Additional Talos configuration patches (advanced users)
# talos_config_patches = [
#   yamlencode({
#     machine = {
#       sysctls = {
#         "vm.swappiness" = "10"
#       }
#     }
#   })
# ]

# Install disk (usually /dev/sda for first disk)
install_disk = "/dev/sda"

# Notes:
# - CRITICAL: Generate and set talos_schematic_id with Longhorn extensions
# - Update node_ip, node_gateway, node_netmask to match your network
# - nfs_server and nfs_path are OPTIONAL (can configure later via Longhorn UI)
# - Update gpu_pci_id based on `lspci` output on your Proxmox host
# - Verify talos_template_name matches the template created by Packer
# - Check node_disk_storage matches your Proxmox storage pool name
# - For multiple nodes, create separate .tfvars files (e.g., node1.tfvars, node2.tfvars)
# - GPU passthrough requires IOMMU enabled in BIOS and GRUB configuration
# - Single-node cluster requires allow_scheduling_on_control_plane = true
# - Longhorn kernel modules and kubelet mounts are auto-configured in main.tf

# ============================================================================
# Post-Deployment: Install Longhorn
# ============================================================================
# After successful `terraform apply`, install Longhorn storage manager:
#
# 1. Verify cluster is ready:
#    kubectl --kubeconfig=./kubeconfig get nodes
#
# 2. Create Longhorn namespace:
#    kubectl create namespace longhorn-system
#    kubectl label namespace longhorn-system pod-security.kubernetes.io/enforce=privileged
#
# 3. Install Longhorn via Helm:
#    helm repo add longhorn https://charts.longhorn.io
#    helm install longhorn longhorn/longhorn \
#      --namespace longhorn-system \
#      --values ../kubernetes/longhorn/longhorn-values.yaml
#
# 4. Apply storage classes:
#    kubectl apply -f ../kubernetes/storage-classes/longhorn-storage-classes.yaml
#
# 5. Verify installation:
#    kubectl get pods -n longhorn-system
#    kubectl get storageclass
#
# Full documentation: ../kubernetes/longhorn/INSTALLATION.md

# ============================================================================
# Traditional VMs Configuration (Optional)
# ============================================================================
# Deploy traditional Linux and Windows VMs from Packer templates
# All VMs are disabled by default - enable as needed

# Common Configuration
# ----------------------------------------------------------------------------

common_tags = ["homelab", "infra"]

# Cloud-init user configuration (applies to all Linux VMs)
cloud_init_user     = "admin"
cloud_init_password = "changeme"  # Change this!
# cloud_init_ssh_keys = ["ssh-rsa AAAA... user@host"]

# Network configuration
default_gateway = "192.168.1.1"
dns_servers     = ["192.168.1.1", "8.8.8.8", "8.8.4.4"]
dns_domain      = "local"

# Ubuntu VM
# ----------------------------------------------------------------------------

deploy_ubuntu_vm      = false  # Set to true to deploy
ubuntu_template_name  = "ubuntu-2404-cloud-template"  # Cloud image template (5-10 min builds)
ubuntu_vm_name        = "ubuntu-dev"
ubuntu_vm_id          = 100
ubuntu_cpu_cores      = 4
ubuntu_memory         = 8192   # MB
ubuntu_disk_size      = 40     # GB
ubuntu_disk_storage   = "local-zfs"
ubuntu_ip_address     = "dhcp"  # Or "192.168.1.100/24" for static
ubuntu_on_boot        = true

# Debian VM
# ----------------------------------------------------------------------------

deploy_debian_vm      = false  # Set to true to deploy
debian_template_name  = "debian-12-cloud-template"  # Cloud image template (5-10 min builds)
debian_vm_name        = "debian-prod"
debian_vm_id          = 200
debian_cpu_cores      = 4
debian_memory         = 8192   # MB
debian_disk_size      = 40     # GB
debian_disk_storage   = "local-zfs"
debian_ip_address     = "dhcp"  # Or "192.168.1.101/24" for static
debian_on_boot        = true

# Arch Linux VM
# ----------------------------------------------------------------------------

deploy_arch_vm      = false  # Set to true to deploy
arch_template_name  = "arch-golden-template-20251118"  # Update with actual template name
arch_vm_name        = "arch-dev"
arch_vm_id          = 300
arch_cpu_cores      = 2
arch_memory         = 4096   # MB
arch_disk_size      = 30     # GB
arch_disk_storage   = "local-zfs"
arch_ip_address     = "dhcp"  # Or "192.168.1.102/24" for static
arch_on_boot        = true

# NixOS VM
# ----------------------------------------------------------------------------

deploy_nixos_vm      = false  # Set to true to deploy
nixos_template_name  = "nixos-golden-template-20251118"  # Update with actual template name
nixos_vm_name        = "nixos-lab"
nixos_vm_id          = 400
nixos_cpu_cores      = 2
nixos_memory         = 4096   # MB
nixos_disk_size      = 30     # GB
nixos_disk_storage   = "local-zfs"
nixos_ip_address     = "dhcp"  # Or "192.168.1.103/24" for static
nixos_on_boot        = true

# Windows Server 2022 VM
# ----------------------------------------------------------------------------

deploy_windows_vm           = false  # Set to true to deploy
windows_template_name       = "windows-server-2022-golden-template-20251118"  # Update with actual template name
windows_vm_name             = "windows-server"
windows_vm_id               = 500
windows_cpu_cores           = 4
windows_memory              = 8192   # MB
windows_disk_size           = 100    # GB
windows_disk_storage        = "local-zfs"
windows_ip_address          = "dhcp"  # Or "192.168.1.104/24" for static
windows_cloud_init_user     = "Administrator"
windows_cloud_init_password = "ChangeMe123!"  # Change this!
windows_on_boot             = true

# ============================================================================
# Traditional VMs Notes
# ============================================================================
#
# Template Names:
# - Update template names after running Packer builds
# - Packer adds timestamps: "ubuntu-24.04-golden-template-20251118-1234"
# - Find templates in Proxmox UI or via: qm list
#
# VM IDs:
# - Must be unique across entire Proxmox cluster
# - Talos typically uses VM ID from Packer (e.g., 10000)
# - Traditional VMs use ranges: Ubuntu 100-199, Debian 200-299, etc.
#
# Resource Allocation:
# - Total available: ~76GB RAM, 10-11 cores (after Proxmox + Talos)
# - Example: Talos=32GB+8c, Ubuntu=16GB+4c, Debian=16GB+4c, Arch=4GB+2c
# - Adjust based on workload requirements
# - See CLAUDE.md for detailed allocation examples
#
# IP Addresses:
# - DHCP: Automatic (requires DHCP server)
# - Static: "192.168.1.100/24" format (update gateway and netmask)
# - Ensure IPs don't conflict with existing devices
#
# Deployment:
# - Deploy all: terraform apply
# - Deploy specific VM: terraform apply -target=module.ubuntu_vm
# - Skip VM: Set deploy_*_vm = false
#
# Cloud-init:
# - All Linux templates have cloud-init pre-installed
# - Windows uses Cloudbase-Init (Windows equivalent)
# - SSH keys recommended for Linux (password auth less secure)
# - Passwords are temporary - change after first login
