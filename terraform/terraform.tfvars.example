# Terraform Variables Example for Talos Linux on Proxmox
#
# Copy this file to terraform.tfvars and fill in your actual values
# cp terraform.tfvars.example terraform.tfvars
#
# NOTE: terraform.tfvars is in .gitignore to prevent committing credentials

# ============================================================================
# Proxmox Connection (Required)
# ============================================================================

proxmox_url      = "https://proxmox.local:8006/api2/json"
proxmox_username = "root@pam"
proxmox_node     = "pve"

# Use API token (recommended)
proxmox_api_token = "PVEAPIToken=terraform@pam!terraform-token=xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx"

# Or use password (less secure)
# proxmox_password = "your-password-here"

# Skip TLS verification for self-signed certs (homelab)
proxmox_insecure = true

# ============================================================================
# Talos Template (Required - created by Packer)
# ============================================================================

talos_template_name = "talos-1.11.4-nvidia-template"
talos_version       = "v1.11.4"
kubernetes_version  = "v1.31.0"

# ============================================================================
# Cluster Configuration
# ============================================================================

cluster_name = "homelab-k8s"

# Cluster endpoint will be set to node_ip if not specified
# cluster_endpoint = "https://192.168.1.100:6443"

# ============================================================================
# Node Configuration (Required)
# ============================================================================

node_name = "talos-node"
node_vm_id = 100

# Static IP configuration (MUST match your network)
node_ip      = "192.168.1.100"  # Change to your desired IP
node_gateway = "192.168.1.1"    # Your router/gateway
node_netmask = 24               # Usually 24 for 255.255.255.0

# ============================================================================
# Hardware Resources
# ============================================================================

# CPU configuration
node_cpu_cores   = 8        # Adjust based on your needs
node_cpu_sockets = 1
node_cpu_type    = "host"   # MUST be 'host' for Talos

# Memory configuration
node_memory = 32768  # 32GB (32768 MB) - adjust as needed

# Disk configuration
node_disk_size    = 200          # 200GB for OS + containers + ephemeral
node_disk_storage = "local-zfs"  # Your Proxmox storage pool name

# ============================================================================
# GPU Passthrough
# ============================================================================

# Enable NVIDIA GPU passthrough
enable_gpu_passthrough = true

# Find your GPU PCI ID with: lspci | grep -i nvidia
# Example output: 01:00.0 VGA compatible controller: NVIDIA Corporation ...
# Use "01:00" format (will be converted to "0000:01:00.0" internally)
gpu_pci_id = "01:00"

# PCIe passthrough mode (recommended)
gpu_pcie = true

# GPU ROM bar (false recommended for GPU passthrough)
gpu_rombar = false

# ============================================================================
# Network Configuration
# ============================================================================

network_bridge = "vmbr0"  # Proxmox bridge name
network_vlan   = 0        # 0 for no VLAN, or your VLAN ID
network_model  = "virtio"

# DNS servers
dns_servers = ["8.8.8.8", "8.8.4.4"]  # Google DNS, or use your own

# NTP servers
ntp_servers = ["time.cloudflare.com"]

# ============================================================================
# External Storage (NFS)
# ============================================================================

# External NAS for persistent storage
nfs_server = "192.168.1.200"       # Your NAS IP address
nfs_path   = "/mnt/tank/k8s-storage"  # NFS export path

# ============================================================================
# Feature Flags
# ============================================================================

# Automatically bootstrap the cluster after creation
auto_bootstrap = true

# Generate kubeconfig file
generate_kubeconfig = true

# Allow pod scheduling on control plane (required for single-node)
allow_scheduling_on_control_plane = true

# Enable QEMU guest agent (should match template)
enable_qemu_agent = true

# Install Cilium CNI (requires manual installation after bootstrap)
install_cilium = true
cilium_version = "1.18.0"

# ============================================================================
# Tags and Metadata
# ============================================================================

tags = ["talos", "kubernetes", "nvidia-gpu", "homelab"]

description = "Talos Linux single-node Kubernetes cluster with NVIDIA GPU support"

# ============================================================================
# Advanced Configuration
# ============================================================================

# Additional Talos configuration patches (advanced users)
# talos_config_patches = [
#   yamlencode({
#     machine = {
#       sysctls = {
#         "vm.swappiness" = "10"
#       }
#     }
#   })
# ]

# Install disk (usually /dev/sda for first disk)
install_disk = "/dev/sda"

# Notes:
# - Update node_ip, node_gateway, node_netmask to match your network
# - Update nfs_server and nfs_path to match your NAS configuration
# - Update gpu_pci_id based on `lspci` output on your Proxmox host
# - Verify talos_template_name matches the template created by Packer
# - Check node_disk_storage matches your Proxmox storage pool name
# - For multiple nodes, create separate .tfvars files (e.g., node1.tfvars, node2.tfvars)
# - GPU passthrough requires IOMMU enabled in BIOS and GRUB configuration
# - Single-node cluster requires allow_scheduling_on_control_plane = true

# ============================================================================
# Traditional VMs Configuration (Optional)
# ============================================================================
# Deploy traditional Linux and Windows VMs from Packer templates
# All VMs are disabled by default - enable as needed

# Common Configuration
# ----------------------------------------------------------------------------

common_tags = ["homelab", "infra"]

# Cloud-init user configuration (applies to all Linux VMs)
cloud_init_user     = "admin"
cloud_init_password = "changeme"  # Change this!
# cloud_init_ssh_keys = ["ssh-rsa AAAA... user@host"]

# Network configuration
default_gateway = "192.168.1.1"
dns_servers     = ["192.168.1.1", "8.8.8.8", "8.8.4.4"]
dns_domain      = "local"

# Ubuntu VM
# ----------------------------------------------------------------------------

deploy_ubuntu_vm      = false  # Set to true to deploy
ubuntu_template_name  = "ubuntu-24.04-golden-template-20251118"  # Update with actual template name
ubuntu_vm_name        = "ubuntu-dev"
ubuntu_vm_id          = 100
ubuntu_cpu_cores      = 4
ubuntu_memory         = 8192   # MB
ubuntu_disk_size      = 40     # GB
ubuntu_disk_storage   = "local-zfs"
ubuntu_ip_address     = "dhcp"  # Or "192.168.1.100/24" for static
ubuntu_on_boot        = true

# Debian VM
# ----------------------------------------------------------------------------

deploy_debian_vm      = false  # Set to true to deploy
debian_template_name  = "debian-12-golden-template-20251118"  # Update with actual template name
debian_vm_name        = "debian-prod"
debian_vm_id          = 200
debian_cpu_cores      = 4
debian_memory         = 8192   # MB
debian_disk_size      = 40     # GB
debian_disk_storage   = "local-zfs"
debian_ip_address     = "dhcp"  # Or "192.168.1.101/24" for static
debian_on_boot        = true

# Arch Linux VM
# ----------------------------------------------------------------------------

deploy_arch_vm      = false  # Set to true to deploy
arch_template_name  = "arch-golden-template-20251118"  # Update with actual template name
arch_vm_name        = "arch-dev"
arch_vm_id          = 300
arch_cpu_cores      = 2
arch_memory         = 4096   # MB
arch_disk_size      = 30     # GB
arch_disk_storage   = "local-zfs"
arch_ip_address     = "dhcp"  # Or "192.168.1.102/24" for static
arch_on_boot        = true

# NixOS VM
# ----------------------------------------------------------------------------

deploy_nixos_vm      = false  # Set to true to deploy
nixos_template_name  = "nixos-golden-template-20251118"  # Update with actual template name
nixos_vm_name        = "nixos-lab"
nixos_vm_id          = 400
nixos_cpu_cores      = 2
nixos_memory         = 4096   # MB
nixos_disk_size      = 30     # GB
nixos_disk_storage   = "local-zfs"
nixos_ip_address     = "dhcp"  # Or "192.168.1.103/24" for static
nixos_on_boot        = true

# Windows Server 2022 VM
# ----------------------------------------------------------------------------

deploy_windows_vm           = false  # Set to true to deploy
windows_template_name       = "windows-server-2022-golden-template-20251118"  # Update with actual template name
windows_vm_name             = "windows-server"
windows_vm_id               = 500
windows_cpu_cores           = 4
windows_memory              = 8192   # MB
windows_disk_size           = 100    # GB
windows_disk_storage        = "local-zfs"
windows_ip_address          = "dhcp"  # Or "192.168.1.104/24" for static
windows_cloud_init_user     = "Administrator"
windows_cloud_init_password = "ChangeMe123!"  # Change this!
windows_on_boot             = true

# ============================================================================
# Traditional VMs Notes
# ============================================================================
#
# Template Names:
# - Update template names after running Packer builds
# - Packer adds timestamps: "ubuntu-24.04-golden-template-20251118-1234"
# - Find templates in Proxmox UI or via: qm list
#
# VM IDs:
# - Must be unique across entire Proxmox cluster
# - Talos typically uses VM ID from Packer (e.g., 10000)
# - Traditional VMs use ranges: Ubuntu 100-199, Debian 200-299, etc.
#
# Resource Allocation:
# - Total available: ~76GB RAM, 10-11 cores (after Proxmox + Talos)
# - Example: Talos=32GB+8c, Ubuntu=16GB+4c, Debian=16GB+4c, Arch=4GB+2c
# - Adjust based on workload requirements
# - See CLAUDE.md for detailed allocation examples
#
# IP Addresses:
# - DHCP: Automatic (requires DHCP server)
# - Static: "192.168.1.100/24" format (update gateway and netmask)
# - Ensure IPs don't conflict with existing devices
#
# Deployment:
# - Deploy all: terraform apply
# - Deploy specific VM: terraform apply -target=module.ubuntu_vm
# - Skip VM: Set deploy_*_vm = false
#
# Cloud-init:
# - All Linux templates have cloud-init pre-installed
# - Windows uses Cloudbase-Init (Windows equivalent)
# - SSH keys recommended for Linux (password auth less secure)
# - Passwords are temporary - change after first login
